{"cells":[{"metadata":{},"cell_type":"markdown","source":"# RIIID - SAKT Transformer Model Inference"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import psutil\nimport joblib\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import riiideducation\n\nenv = riiideducation.make_env()\niter_test = env.iter_test()","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_SAMPLES = 320000\nMAX_SEQ = 180\nMIN_SAMPLES = 5\nEMBED_DIM = 128\nDROPOUT_RATE = 0.2\nLEARNING_RATE = 2e-3\nMAX_LEARNING_RATE = 2e-3\nEPOCHS = 10\nTRAIN_BATCH_SIZE = 64\nACCEPTED_USER_CONTENT_SIZE = 4","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FFN(nn.Module):\n    def __init__(self, state_size=200, forward_expansion=1, bn_size=MAX_SEQ-1, dropout=0.2):\n        super(FFN, self).__init__()\n        self.state_size = state_size\n        \n        self.lr1 = nn.Linear(state_size, forward_expansion * state_size)\n        self.relu = nn.ReLU()\n        self.bn = nn.BatchNorm1d(bn_size)\n        self.lr2 = nn.Linear(forward_expansion * state_size, state_size)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        x = self.relu(self.lr1(x))\n        x = self.bn(x)\n        x = self.lr2(x)\n        return self.dropout(x)\n\n\ndef future_mask(seq_length):\n    future_mask = (np.triu(np.ones([seq_length, seq_length]), k = 1)).astype('bool')\n    return torch.from_numpy(future_mask)\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, heads=8, dropout=DROPOUT_RATE, forward_expansion=1):\n        super(TransformerBlock, self).__init__()\n        self.multi_att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=heads, dropout=dropout)\n        self.dropout = nn.Dropout(dropout)\n        self.layer_normal = nn.LayerNorm(embed_dim)\n        self.ffn = FFN(embed_dim, forward_expansion = forward_expansion, dropout=dropout)\n        self.layer_normal_2 = nn.LayerNorm(embed_dim)\n        \n\n    def forward(self, value, key, query, att_mask):\n        att_output, att_weight = self.multi_att(value, key, query, attn_mask=att_mask)\n        att_output = self.dropout(self.layer_normal(att_output + value))\n        att_output = att_output.permute(1, 0, 2) # att_output: [s_len, bs, embed] => [bs, s_len, embed]\n        x = self.ffn(att_output)\n        x = self.dropout(self.layer_normal_2(x + att_output))\n        return x.squeeze(-1), att_weight\n    \n\nclass Encoder(nn.Module):\n    def __init__(self, n_skill, max_seq=100, embed_dim=128, part_dim=7, dropout=DROPOUT_RATE, forward_expansion=1, num_layers=1, heads = 8):\n        super(Encoder, self).__init__()\n        self.n_skill, self.embed_dim = n_skill, embed_dim\n        self.embedding = nn.Embedding(2 * n_skill + 1, embed_dim)\n        self.pos_embedding = nn.Embedding(max_seq - 1, embed_dim)\n        self.e_embedding = nn.Embedding(n_skill + 1, embed_dim)\n        self.part_embedding = nn.Embedding(n_skill + 1, part_dim)            # TOEIC has 7 parts\n        self.e_part_embedding = nn.Linear(embed_dim + part_dim, embed_dim)\n\n        self.layers = nn.ModuleList([TransformerBlock(embed_dim, forward_expansion = forward_expansion) for _ in range(num_layers)])\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, question_ids, parts):\n        device = x.device\n        x = self.embedding(x)\n\n        pos_id = torch.arange(x.size(1)).unsqueeze(0).to(device)\n        pos_x = self.pos_embedding(pos_id)\n        \n        x = self.dropout(x + pos_x)\n        x = x.permute(1, 0, 2) # x: [bs, s_len, embed] => [s_len, bs, embed]\n\n        e = self.e_embedding(question_ids)\n        e = e.permute(1, 0, 2)\n        embedded_parts = self.part_embedding(parts)\n        embedded_parts = embedded_parts.permute(1, 0, 2)\n\n        # Project meta parts and content id to dim\n        meta_question = torch.cat((e, embedded_parts), dim=2)\n        meta_question = self.e_part_embedding(meta_question)\n\n        for layer in self.layers:\n            att_mask = future_mask(e.size(0)).to(device)\n            x, att_weight = layer(e, x, x, att_mask=att_mask)\n            x = x.permute(1, 0, 2)\n        x = x.permute(1, 0, 2)\n        return x, att_weight\n\n\nclass SAKTModel(nn.Module):\n    def __init__(self, n_skill, max_seq=100, embed_dim=128, part_dim=7, dropout=DROPOUT_RATE, forward_expansion = 1, enc_layers=1, heads = 8):\n        super(SAKTModel, self).__init__()\n        self.encoder = Encoder(n_skill, max_seq, embed_dim, part_dim, dropout, forward_expansion, num_layers=enc_layers)\n        self.pred = nn.Linear(embed_dim, 1)\n        \n    def forward(self, x, question_ids, parts):\n        x, att_weight = self.encoder(x, question_ids, parts)\n        x = self.pred(x)\n        return x.squeeze(-1), att_weight","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Pretrained Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"skills = joblib.load(\"/kaggle/input/sakt-transformer/skills.pkl.zip\")\nn_skill = len(skills)\ngroup = joblib.load(\"/kaggle/input/sakt-transformer/group.pkl.zip\")","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Question csv\nquestion_dtypes = { \n            'question_id': 'int16', \n            'part': 'int16'\n         }\nquestion_df = pd.read_csv('/kaggle/input/riiid-test-answer-prediction/questions.csv')[question_dtypes.keys()]\nfor col, dtype in question_dtypes.items():\n    question_df[col] = question_df[col].astype(dtype)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = SAKTModel(n_skill, max_seq=MAX_SEQ, embed_dim=EMBED_DIM, dropout=DROPOUT_RATE)\ntry:\n    model.load_state_dict(torch.load(\"/kaggle/input/sakt-transformer/sakt_model_final.pt\"))\nexcept:\n    model.load_state_dict(torch.load(\"/kaggle/input/sakt-transformer/sakt_model_final.pt\", map_location='cpu'))\nmodel.to(device)\nmodel.eval()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"SAKTModel(\n  (encoder): Encoder(\n    (embedding): Embedding(27047, 128)\n    (pos_embedding): Embedding(179, 128)\n    (e_embedding): Embedding(13524, 128)\n    (part_embedding): Embedding(13524, 7)\n    (e_part_embedding): Linear(in_features=135, out_features=128, bias=True)\n    (layers): ModuleList(\n      (0): TransformerBlock(\n        (multi_att): MultiheadAttention(\n          (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n        )\n        (dropout): Dropout(p=0.2, inplace=False)\n        (layer_normal): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n        (ffn): FFN(\n          (lr1): Linear(in_features=128, out_features=128, bias=True)\n          (relu): ReLU()\n          (bn): BatchNorm1d(179, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (lr2): Linear(in_features=128, out_features=128, bias=True)\n          (dropout): Dropout(p=0.2, inplace=False)\n        )\n        (layer_normal_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (dropout): Dropout(p=0.2, inplace=False)\n  )\n  (pred): Linear(in_features=128, out_features=1, bias=True)\n)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, samples, test_df, question_df, n_skill, max_seq=100):\n        super(TestDataset, self).__init__()\n#         self.samples, self.user_ids, self.test_df = samples, [x for x in test_df[\"user_id\"].unique()], test_df\n#         self.n_skill, self.max_seq = n_skill, max_seq\n        \n        self.samples = samples\n        self.user_ids = [x for x in test_df[\"user_id\"].unique()]\n        self.test_df = test_df\n        self.question_df = question_df\n        self.n_skill = len(skills)\n        self.max_seq = max_seq\n\n    def __len__(self):\n        return self.test_df.shape[0]\n    \n    def __getitem__(self, index):\n        test_info = self.test_df.iloc[index]\n        \n        user_id = test_info['user_id']\n        target_id = test_info['content_id']\n        target_part = self.question_df['part'][target_id]\n        \n        content_id_seq = np.zeros(self.max_seq, dtype=int)\n        answered_correctly_seq = np.zeros(self.max_seq, dtype=int)\n        parts_seq = np.zeros(self.max_seq, dtype=int)\n        \n        if user_id in self.samples.index:\n            content_id, answered_correctly = self.samples[user_id]\n            parts = self.question_df.loc[content_id, 'part'].to_list()\n            seq_len = len(content_id)\n            \n            if seq_len >= self.max_seq:\n                content_id_seq = content_id[-self.max_seq:]\n                answered_correctly_seq = answered_correctly[-self.max_seq:]\n                parts_seq = parts[-self.max_seq:]\n            else:\n                content_id_seq[-seq_len:] = content_id\n                answered_correctly_seq[-seq_len:] = answered_correctly\n                parts_seq[-seq_len:] = parts\n                \n        x = content_id_seq[1:].copy()\n        x += (answered_correctly_seq[1:] == 1) * self.n_skill\n        \n        questions = np.append(content_id_seq[2:], [target_id])\n        parts_id = np.append(parts_seq[2:], [target_part])\n        \n        return x, questions, parts_id","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prev_test_df = None\n\nfor (test_df, sample_prediction_df) in tqdm(iter_test):\n    \n    if (prev_test_df is not None) & (psutil.virtual_memory().percent<90):\n        print(psutil.virtual_memory().percent)\n        \n        prev_test_df['answered_correctly'] = eval(test_df['prior_group_answers_correct'].iloc[0])\n        prev_test_df = prev_test_df[prev_test_df.content_type_id == False]\n        prev_group = prev_test_df[['user_id', 'content_id', 'answered_correctly']].groupby('user_id').apply(lambda r: (\n            r['content_id'].values,\n            r['answered_correctly'].values))\n        \n        for prev_user_id in prev_group.index:\n            prev_group_content = prev_group[prev_user_id][0]\n            prev_group_answered_correctly = prev_group[prev_user_id][1]\n            if prev_user_id in group.index:\n                group[prev_user_id] = (np.append(group[prev_user_id][0], prev_group_content), \n                                       np.append(group[prev_user_id][1], prev_group_answered_correctly))\n            else:\n                group[prev_user_id] = (prev_group_content, prev_group_answered_correctly)\n            \n            if len(group[prev_user_id][0]) > MAX_SEQ:\n                new_group_content = group[prev_user_id][0][-MAX_SEQ:]\n                new_group_answered_correctly = group[prev_user_id][1][-MAX_SEQ:]\n                group[prev_user_id] = (new_group_content, new_group_answered_correctly)\n                \n    prev_test_df = test_df.copy()\n    test_df = test_df[test_df.content_type_id == False]\n    \n    test_dataset = TestDataset(group, test_df, question_df, n_skill, max_seq=MAX_SEQ)\n    test_dataloader = DataLoader(test_dataset, batch_size=len(test_df), shuffle=False)\n    \n    item = next(iter(test_dataloader))\n    x = item[0].to(device).long()\n    target_id = item[1].to(device).long()\n    parts = item[2].to(device).long()\n    \n    with torch.no_grad():\n        output, _ = model(x, target_id, parts)\n        \n    output = torch.sigmoid(output)\n    output = output[:, -1]\n    test_df['answered_correctly'] = output.cpu().numpy()\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])","execution_count":9,"outputs":[{"output_type":"stream","text":"1it [00:00,  4.14it/s]","name":"stderr"},{"output_type":"stream","text":"179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n14.7\n","name":"stdout"},{"output_type":"stream","text":"\r2it [00:00,  4.20it/s]","name":"stderr"},{"output_type":"stream","text":"179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n14.8\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n","name":"stdout"},{"output_type":"stream","text":"4it [00:00,  5.14it/s]","name":"stderr"},{"output_type":"stream","text":"15.0\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n179 179 179\n","name":"stdout"},{"output_type":"stream","text":"4it [00:01,  2.81it/s]\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"All the credits go to this popular notebook https://www.kaggle.com/leadbest/sakt-with-randomization-state-updates which is a modification of https://www.kaggle.com/wangsg/a-self-attentive-model-for-knowledge-tracing. Please show some support to these original work kernels."},{"metadata":{},"cell_type":"markdown","source":"---"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}